---
title: "Lab 3"
author: "Jonathan Eng"
output: pdf_document
date: "11:59PM Saturday, February 22, 2020"
---

## Review from Lab 2

You have a set of names divided by gender (M / F) and generation (Boomer / GenX / Millenial):

* M / Boomer      "Theodore, Bernard, Gene, Herbert, Ray, Tom, Lee, Alfred, Leroy, Eddie"
* M / GenX        "Marc, Jamie, Greg, Darryl, Tim, Dean, Jon, Chris, Troy, Jeff"
* M / Millennial  "Zachary, Dylan, Christian, Wesley, Seth, Austin, Gabriel, Evan, Casey, Luis"
* F / Boomer      "Gloria, Joan, Dorothy, Shirley, Betty, Dianne, Kay, Marjorie, Lorraine, Mildred"
* F / GenX        "Tracy, Dawn, Tina, Tammy, Melinda, Tamara, Tracey, Colleen, Sherri, Heidi"
* F / Millennial  "Samantha, Alexis, Brittany, Lauren, Taylor, Bethany, Latoya, Candice, Brittney, Cheyenne"

Create a list-within-a-list that will intelligently store this data.

```{r}
set <- list("M" = list(), "F" = list())
set$M$Boomer = strsplit("Theodore, Bernard, Gene, Herbert, Ray, Tom, Lee, Alfred, Leroy, Eddie", split = ", ")[[1]]
set$M$GenX = strsplit("Marc, Jamie, Greg, Darryl, Tim, Dean, Jon, Chris, Troy, Jeff", split = ", ")[[1]]
set$M$Millennial = strsplit("Zachary, Dylan, Christian, Wesley, Seth, Austin, Gabriel, Evan, Casey, Luis", split = ", ")[[1]]
set$F$Boomer = strsplit("Gloria, Joan, Dorothy, Shirley, Betty, Dianne, Kay, Marjorie, Lorraine, Mildred", split = ", ")[[1]]
set$F$GenX = strsplit("Tracy, Dawn, Tina, Tammy, Melinda, Tamara, Tracey, Colleen, Sherri, Heidi", split = ", ")[[1]]
set$F$Millennial = strsplit("Samantha, Alexis, Brittany, Lauren, Taylor, Bethany, Latoya, Candice, Brittney, Cheyenne", split = ", ")[[1]]
set
```


Imagine you are running an experiment with many manipulations. You have 14 levels in the variable "treatment" with levels a, b, c, etc. For each of those manipulations you have 3 submanipulations in a variable named "variation" with levels A, B, C. Then you have "gender" with levels M / F. Then you have "generation" with levels Boomer, GenX, Millenial. Then you will have 6 runs per each of these groups. In each set of 6 you will need to select a name without duplication from the appropriate set of names (from the last question). Create a data frame with columns treatment, variation, gender, generation, name and y that will store all the unique unit information in this experiment. Leave y empty because it will be measured as the experiment is executed.

```{r}
n = 14 * 3 * 2 * 3 * 10
X = data.frame(treatment = rep(NA,n), 
               variation = rep(NA,n),
               gender = rep(NA,n),
               generation = rep(NA,n),
               name = rep(NA,n),
               y = rep(NA, n))
X$treatment = rep(letters[1:14], each = n / 14)
X$variation = rep(rep(LETTERS[1:3], each = n / 14 / 3) , times = 14)
X$gender = rep(rep(c("M", "F"), each = n / 14 / 3 / 2), times = 14*3)
X$generation = rep(rep(c("Boomer", "GenX", "Millenial"), each = n / 14/ 3 /2 / 3), times = 14 * 3 * 2)
X$name = rep(unlist(set), times = 14 * 3)
X
```

## Packages

Install the package `pacman` using regular base R.

```{r}
install.packages("pacman")
```


First, install the package `testthat` (a widely accepted testing suite for R) from https://github.com/r-lib/testthat using `pacman`. If you are using Windows, this will be a long install, but you have to go through it for some of the stuff we are doing in class. LINUX (or MAC) is preferred for coding. If you can't get it to work, install this package from CRAN (still using `pacman`), but this is not recommended long term.

```{r}
pacman::p_load(testthat)
```

* Create vector `v` consisting of all numbers from -100 to 100 and test using the second line of code:

```{r}
v = seq(-100, 100)
expect_equal(v, -100 : 100)
```

If there are any errors, the `expect_equal` function will tell you about them. If there are no errors, then it will be silent.

Test the `my_reverse` function from lab2 using the following code:

```{r}
expect_equal(my_reverse(v), rev(v))
expect_equal(my_reverse(c("A", "B", "C")), c("C", "B", "A"))
```

## Basic Binary Classification Modeling

* Load the famous `iris` data frame into the namespace. Provide a summary of the columns and write a few descriptive sentences about the distributions using the code below and in English.

```{r}
data("iris")
iris
```

The outcome metric is `Species`. This is what we will be trying to predict. However, we only care about binary classification between "setosa" and "versicolor" for the purposes of this exercise. Thus the first order of business is to drop one class. Let's drop the data for the level "virginica" from the data frame.

```{r}
iris = iris[iris$Species != "virginica", ]
table(iris$Species)
iris
```

Now create a vector `y` that is length the number of remaining rows in the data frame whose entries are 0 if "setosa" and 1 if "versicolor".

```{r}
y = as.numeric(iris$Species == "versicolor")
y
```

* Write a function `mode` returning the sample mode.

```{r}
mode = function(x){
  y = unique(x)
  y[which.max(tabulate(match(x, y)))]
}
```

* Fit a threshold model to `y` using the feature `Sepal.Length`. Write your own code to do this. What is the estimated value of the threshold parameter? What is the total number of errors this model makes?

```{r}
n = nrow(iris)
num_errors = array(NA, n)
for (i in 1 : n) {
  y_hat = as.numeric(iris$Sepal.Length > iris$Sepal.Length[i])
  num_errors[i] = sum(y_hat != y)
}

threshold = iris$Sepal.Length[which.min(num_errors)] 

g = function(x) {
  as.numeric(x > threshold)
  }

sum(g(iris$Sepal.Length) != y)
```

Does Robin's threshold model's performance make sense given the following summaries:

```{r}
threshold
summary(iris[iris$Species == "setosa", "Sepal.Length"])
summary(iris[iris$Species == "versicolor", "Sepal.Length"])
```

Write your answer here in English.

Robin's threshhold model's performance makes sense because the median of "Setosa" is below his predicted 5.4 length threshold and the average for "Versicolor" is above his 5.4 length threshold.

Create the function `g` explicitly that can predict `y` from `x` being a new `Sepal.Length`.

```{r}
g = function(x) {
  as.numeric(x > threshold)
  }
```

* What is the total number of errors this model makes in the dataset $\mathbb{D}$?

```{r}
sum(g(iris$Sepal.Length) != y)
```


## Perceptron

You will code the "perceptron learning algorithm". Take a look at the comments above the function. This is standard "Roxygen" format for documentation. Hopefully, we will get to packages at some point and we will go over this again. It is your job also to fill in this documentation.

```{r}
#' TO-DO: perceptron_learning_algorithm
#'
#' TO-DO: By taking in historial data to train and improve impon classification abilities, the algorithm is able to predict future values based on weights on factors learned from the training data.
#'
#' @param Xinput      Concatenated columms of a dataset
#' @param y_binary    holds binary values 0 and 1
#' @param MAX_ITER    Amount of runs for the perception function to adjust its values
#' @param w           W defult filled with NULL as a bug checker
#'
#' @return            The computed final parameter (weight) as a vector of length p + 1
#' @export            [In a package, this documentation parameter signifies this function becomes a public method. Leave this blank.]
#'
#' @author            [Jonathan Eng]
perceptron_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 100, w = NULL){
  X = cbind(1, Xinput)
  n = nrow(X)
  p = ncol(X)
  w = rep(0, p)
  for (iter in 1 : MAX_ITER){
    is_error = FALSE
    for(a in 1 : n){
      x_a = X[a, ]
      yhat_i = ifelse(sum(X[a, ] * w) > 0, 1, 0)

      if(yhat_i == 0)
        is_error = TRUE

      for (i in 1 : p){
        y_i = y_binary[a]
        w[i] = w[i] + (y_i - yhat_i) * x_a[i]
      }
    }
    if(is_error == FALSE)
      return(w)
  }
  w
}
```

To understand what the algorithm is doing - linear "discrimination" between two response categories, we can draw a picture. First let's make up some very simple training data $\mathbb{D}$.

```{r}
Xy_simple = data.frame(
 response = factor(c(0, 0, 0, 1, 1, 1)), #nominal
 first_feature = c(1, 1, 2, 3, 3, 4),    #continuous
 second_feature = c(1, 2, 1, 3, 4, 3)    #continuous
)
```

We haven't spoken about visualization yet, but it is important we do some of it now. Thus, I will write this code for you and you will just run it. First we load the visualization library we're going to use:

```{r}
pacman::p_load(ggplot2)
```

We are going to just get some plots and not talk about the code to generate them as we will have a whole unit on visualization using `ggplot2` in the future.

Let's first plot $y$ by the two features so the coordinate plane will be the two features and we use different colors to represent the third dimension, $y$.

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

TO-DO: Explain this picture.
This picture plots 6 points with the first feature along the horizontal axis and the second feature on the vertical axis. It appears that those with either two 3's or a combination of 3 and 4 for feature 1 and 2 shows a response. Those with two 1's or a combination of 1 and 2 shows no response. Response and No Response appear to be linearally separatable with the given picture.

Now, let us run the algorithm and see what happens:

```{r}
w_vec_simple_per = perceptron_learning_algorithm(
  cbind(Xy_simple$first_feature, Xy_simple$second_feature),
  as.numeric(Xy_simple$response == 1))
w_vec_simple_per
```

Explain this output. What do the numbers mean? What is the intercept of this line and the slope? You will have to do some algebra.

The output values are the weights to corresponding x variables. The weight would be farther from 0 (in both the negative and positive direction) if the variable has a stronger influence on the phenonemoun 


```{r}
simple_perceptron_line = geom_abline(
    intercept = -w_vec_simple_per[1] / w_vec_simple_per[3], 
    slope = -w_vec_simple_per[2] / w_vec_simple_per[3], 
    color = "orange")
simple_viz_obj + simple_perceptron_line
```

Explain this picture. Why is this line of separation not "satisfying" to you?

The line separation is not satisfying because the line appeares to be closer to the reponse 0 red points than the response 1 blue points. Additionally, from looking at the graph it seems that a line with a diagonal negative slope woudl fit better compared to the current positive slope.

For extra credit, program the maximum-margin hyperplane perceptron that provides the best linear discrimination model for linearly separable data. Make sure you provide ROxygen documentation for this function.

```{r}
#TO-DO
```


## Support Vector Machine


```{r}
X_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
y_binary = as.numeric(Xy_simple$response == 1)
```

Use the `e1071` package to fit an SVM model to `y_binary` using the features in `X_simple_feature_matrix`. Do not specify the $\lambda$ (i.e. do not specify the `cost` argument). Call the model object `svm_model`. Otherwise the remaining code won't work.

```{r}
pacman::p_load(e1071)
Xy_simple = data.frame(
 first_feature = c(1, 1, 2, 3, 3, 4),    #continuous
 second_feature = c(1, 2, 1, 3, 4, 3)    #continuous
)

svm_model = svm(X_simple_feature_matrix, y_binary, kernel = "linear", scale = FALSE)
svm_model
```

and then use the following code to visualize the line in purple:

```{r}
w_vec_simple_svm = c(
  svm_model$rho, #the b term
  -t(svm_model$coefs) %*% X_simple_feature_matrix[svm_model$index, ] # the other terms
)
simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_viz_obj + simple_perceptron_line + simple_svm_line
```

Is this SVM line a better fit than the perceptron?

This SVM line is a worse fit compared to the perceptron. Had the svm line be pushed more towards the center it would be a better fit compared to the perceptron. 

3. Now write pseuocode for your own implementation of the linear support vector machine algorithm using the Vapnik objective function we discussed.

Note there are differences between this spec and the perceptron learning algorithm spec in question \#1. You should figure out a way to respect the `MAX_ITER` argument value. 


```{r}
#' Support Vector Machine 
#
#' This function implements the hinge-loss + maximum margin linear support vector machine algorithm of Vladimir Vapnik (1963).
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param MAX_ITER    The maximum number of iterations the algorithm performs. Defaults to 5000.
#' @param lambda      A scalar hyperparameter trading off margin of the hyperplane versus average hinge loss.
#'                    The default value is 1.
#' @return            The computed final parameter (weight) as a vector of length p + 1
linear_svm_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 5000, lambda = 0.1){
  #TO-DO: write pseudo code in comments
#   vector = dimension of Xinput columns
#   for i to MAX_ITER{
#     x_i = Xinput[row i]
#     yhat_i = if x_i * vector = 0 then yhat_i = 1, otherwise yhat_i = 1
#     y_i = y_binary[ ith value]
#     vector[i] = vector[i]+(y_i - yhat_i) * x_i[column 1 to last column]
#   }
#   return vector
 }
```


If you are enrolled in 390 the following is extra credit but if you're enrolled in 650, the following is required. Write the actual code. You may want to take a look at the `optimx` package we discussed in class. You can feel free to define another function (a "private" function) in this chunk if you wish. R has a way to create public and private functions, but I believe you need to create a package to do that (beyond the scope of this course).

```{r}
#' This function implements the hinge-loss + maximum margin linear support vector machine algorithm of Vladimir Vapnik (1963).
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param MAX_ITER    The maximum number of iterations the algorithm performs. Defaults to 5000.
#' @param lambda      A scalar hyperparameter trading off margin of the hyperplane versus average hinge loss.
#'                    The default value is 1.
#' @return            The computed final parameter (weight) as a vector of length p + 1
linear_svm_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 5000, lambda = 0.1){
  #TO-DO
}
```

If you wrote code (the extra credit), run your function using the defaults and plot it in brown vis-a-vis the previous model's line:

```{r}
svm_model_weights = linear_svm_learning_algorithm(X_simple_feature_matrix, y_binary)
my_svm_line = geom_abline(
    intercept = svm_model_weights[1] / svm_model_weights[3],#NOTE: negative sign removed from intercept argument here
    slope = -svm_model_weights[2] / svm_model_weights[3], 
    color = "brown")
simple_viz_obj  + my_svm_line
```

Is this the same as what the `e1071` implementation returned? Why or why not?

4. Write a $k=1$ nearest neighbor algorithm using the Euclidean distance function. Respect the spec below:

```{r}
#' This function implements the nearest neighbor algorithm.
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param Xtest       The test data that the algorithm will predict on as a n* x p matrix.
#' @return            The predictions as a n* length vector.
nn_algorithm_predict = function(Xinput, y_binary, Xtest){
  #TO-DO
}
```

Write a few tests to ensure it actually works:

```{r}
#TO-DO
```

We now add an argument `d` representing any legal distance function to the `nn_algorithm_predict` function. Update the implementation so it performs NN using that distance function. Set the default function to be the Euclidean distance in the original function. Also, alter the documentation in the appropriate places.

```{r}
#TO-DO
```

For extra credit (unless you're a masters student), add an argument `k` to the `nn_algorithm_predict` function and update the implementation so it performs KNN. In the case of a tie, choose $\hat{y}$ randomly. Set the default `k` to be the square root of the size of $\mathcal{D}$ which is an empirical rule-of-thumb popularized by the "Pattern Classification" book by Duda, Hart and Stork (2007). Also, alter the documentation in the appropriate places.

```{r}
#TO-DO --- extra credit for undergrads